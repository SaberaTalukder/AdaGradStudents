{"cells":[{"metadata":{},"cell_type":"markdown","source":"* The temporal order is scrambled in the test data making TS useless there.\n* Still, just for learning / realism, we can still do it in the training data! \n* Let's add pseudo dates, and aggregate features on column subsets. Finally i'll run a model to predict the target!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom datetime import datetime\nfrom scipy.special import logsumexp\n\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, classification_report","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/caltech-cs155-2020/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/caltech-cs155-2020/test.csv\")\ndf = pd.concat([train,test],sort=False)\nprint(df.shape)\nprint(df.columns)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## y is binary.\ndisplay(train[\"y\"].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bid_cols = ['bid1','bid2', 'bid3', 'bid4', 'bid5']\nbid_vol_cols = ['bid1vol', 'bid2vol', 'bid3vol', 'bid4vol', 'bid5vol']\nask_cols = ['ask1', 'ask2', 'ask3', 'ask4', 'ask5',]\nask_vol_cols = ['ask1vol','ask2vol', 'ask3vol', 'ask4vol', 'ask5vol']\n\ngroup_cols = {\"bid_cols\":bid_cols,\"bid_vol_cols\":bid_vol_cols,\"ask_cols\":ask_cols,\"ask_vol_cols\":ask_vol_cols}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Additional features could include: rank, which bid number is the max/min, etc' \n* features between the aggregated features (e.g. max bid div max ask..)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for group in group_cols.keys():\n    print(group)\n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    df[f\"{group}_min\"] = df[group_cols[group]].min(axis=1)\n    df[f\"{group}_spread\"] = df[f\"{group}_max\"].div(df[f\"{group}_min\"])\n    df[f\"{group}_logsumexp\"] = df[group_cols[group]].apply(logsumexp)\n    \n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    \ndf[\"last_price_div__mid\"] = df[\"last_price\"].div(df[\"mid\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"date\"] = pd.to_datetime(\"1.1.2019\")\ndf[\"date\"] = df[\"date\"] + pd.to_timedelta(df[\"id\"]/2,unit=\"s\") # 500 ms per row\n\ndf[\"date\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split back into train and test, and build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.loc[~df.y.isna()]\nprint(f\"train shape {train.shape[0]}\")\ntest = df.loc[df.y.isna()]\nprint(f\"test shape {test.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"id\"],axis=1).to_csv(\"train_hft.csv.gz\",index=False,compression=\"gzip\")\ntest.to_csv(\"test_hft_nodates.csv.gz\",index=False,compression=\"gzip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we don't know if the test set has a temporal split, so we'll just try a random split for now\nX = train.drop([\"id\",\"date\",\"y\"],axis=1)\ny = train[\"y\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pool = Pool(data=X,label = y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ### hyperparameter tuning example grid for catboost : \n# grid = {'learning_rate': [0.05, 0.1],\n#         'depth': [6, 11],\n# #         'l2_leaf_reg': [1, 3,9],\n# #        \"iterations\": [1000],\n#        \"custom_metric\":['Logloss', 'AUC']}\n\n# model = CatBoostClassifier()\n\n# ## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`\n# grid_search_result = model.grid_search(grid, \n#                                        train_pool,\n#                                        plot=True,\n#                                        refit = True, #  refit best model on all data\n#                                       partition_random_seed=42)\n\n# print(model.get_best_score())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostClassifier()\n    \nmodel.fit(train_pool, plot=True,silent=True)\nprint(model.get_best_score())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Features importances\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score > 0.2:\n        print('{0}: {1:.2f}'.format(name, score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_pool)\n\n# visualize the training set predictions\n# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \nshap.force_plot(explainer.expected_value,shap_values[0,:300], X.iloc[0,:300])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## todo : PDP features +- from shap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## export predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Predicted\"] = model.predict(test.drop([\"id\",\"date\",\"y\"],axis=1),prediction_type='Probability')[:,1]\ntest[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}